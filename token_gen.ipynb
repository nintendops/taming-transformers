{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read checkpoint path logs/2022-07-28T00-02-08_usc_pretrained_vggan/checkpoints/last.ckpt for dqgan\n",
      "Read checkpoint path logs/2022-08-01T01-37-16_use_2048_cond_stage/checkpoints/last.ckpt for cond dqgan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import importlib\n",
    "import yaml\n",
    "import albumentations\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.notebook import tqdm\n",
    "from taming.data.base import ImagePaths, NumpyPaths, ConcatDatasetWithIndex\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "# dataset_root_path = \"\"\n",
    "# source_dataset = \"\"\n",
    "# dataset_name = \"\"\n",
    "\n",
    "# load config for transformer\n",
    "config_path = \"configs/owt_transformer.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "vggan_config = config.model.params.first_stage_config\n",
    "cond_vggan_config = config.model.params.cond_stage_config\n",
    "\n",
    "vggan_ckpt_path = vggan_config.params.ckpt_path\n",
    "cond_vggan_ckpt_path = cond_vggan_config.params.ckpt_path\n",
    "\n",
    "print(f\"Read checkpoint path {vggan_ckpt_path} for dqgan\")\n",
    "print(f\"Read checkpoint path {cond_vggan_ckpt_path} for cond dqgan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Restored from logs/2022-07-28T00-02-08_usc_pretrained_vggan/checkpoints/last.ckpt\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Restored from logs/2022-08-01T01-37-16_use_2048_cond_stage/checkpoints/last.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "device = torch.device('cuda')\n",
    "\n",
    "vggan = instantiate_from_config(vggan_config).to(device)\n",
    "vggan_cond = instantiate_from_config(cond_vggan_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'base_learning_rate': 4.5e-06, 'target': 'taming.models.cond_transformer.Net2NetTransformer', 'params': {'cond_stage_key': 'segmentation', 'transformer_config': {'target': 'taming.modules.transformer.mingpt.CondGPT', 'params': {'vocab_size': 8192, 'vocab_size_cond': 1024, 'block_size': 512, 'n_layer': 40, 'n_head': 16, 'n_embd': 1408, 'embd_pdrop': 0.1, 'resid_pdrop': 0.1, 'attn_pdrop': 0.1}}, 'first_stage_config': {'target': 'taming.models.vqgan.VQModel', 'params': {'ckpt_path': 'logs/2022-07-28T00-02-08_usc_pretrained_vggan/checkpoints/last.ckpt', 'embed_dim': 256, 'n_embed': 8192, 'ddconfig': {'double_z': False, 'z_channels': 256, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [16], 'dropout': 0.0}, 'lossconfig': {'target': 'taming.modules.losses.DummyLoss'}}}, 'cond_stage_config': {'target': 'taming.models.vqgan.VQSegmentationModel', 'params': {'ckpt_path': 'logs/2022-08-01T01-37-16_use_2048_cond_stage/checkpoints/last.ckpt', 'embed_dim': 128, 'n_embed': 1024, 'image_key': 'segmentation', 'n_labels': 3, 'ddconfig': {'double_z': False, 'z_channels': 256, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [16], 'dropout': 0.0}, 'lossconfig': {'target': 'taming.modules.losses.DummyLoss'}}}}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 2, 'train': {'target': 'taming.data.owt.OWTBase', 'params': {'size': 2048, 'dataroot': '/home/ICT2000/chenh/Haiwei/Datasets/OWT/USC_Galen_Center', 'crop_size': 256, 'onehot_segmentation': True}}}}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aa3b23b8214a35ad72b43921dff469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataloader\n",
    "size = config.data.params.train.params.size\n",
    "dataroot = config.data.params.train.params.dataroot\n",
    "datasetname = f\"tokenized_{size}\"\n",
    "os.makedirs(os.path.join(dataroot, datasetname),exist_ok=True)\n",
    "rescaler = albumentations.SmallestMaxSize(max_size=size)\n",
    "# cropper = albumentations.CenterCrop(height=self.crop_size, width=self.crop_size)\n",
    "# preprocessor = albumentations.Compose([self.rescaler, self.cropper],additional_targets={\"segmentation\": \"image\"})\n",
    "# rescaler = albumentations.Resize(height=self.size, width=self.size)\n",
    "preprocessor = albumentations.Compose([rescaler], additional_targets={\"segmentation\": \"image\"})\n",
    "\n",
    "def preprocess_image(image_path, segmentation_path, onehot=True):\n",
    "    image = Image.open(image_path)\n",
    "    if not image.mode == \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "\n",
    "    segmentation = np.load(segmentation_path)\n",
    "    segmentation = segmentation * (segmentation>-1)\n",
    "    segmentation = segmentation.astype(np.uint8)\n",
    "    processed = preprocessor(image=image, segmentation=segmentation)\n",
    "    image, segmentation = processed[\"image\"], processed[\"segmentation\"]\n",
    "    image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "    if onehot:\n",
    "        assert segmentation.dtype == np.uint8\n",
    "        # make it one hot\n",
    "        n_labels = 3\n",
    "        flatseg = np.ravel(segmentation)\n",
    "        onehot = np.zeros((flatseg.size, n_labels), dtype=np.bool)\n",
    "        onehot[np.arange(flatseg.size), flatseg] = True\n",
    "        onehot = onehot.reshape(segmentation.shape + (n_labels,)).astype(int)\n",
    "        segmentation = onehot\n",
    "    else:\n",
    "        # normalizing to (-1, 1)\n",
    "        segmentation = (segmentation / 1.0 - 1.0).astype(np.float32)\n",
    "\n",
    "    return image, segmentation\n",
    "\n",
    "def patch_encode(x, model):\n",
    "    h,w = x.shape[:2]\n",
    "    index_grid = np.zeros([h//16, w//16],dtype=np.int64)\n",
    "    grid_size = crop_size // 16\n",
    "    for i in range(0,h,crop_size):\n",
    "        for j in range(0,w,crop_size):\n",
    "            if i + crop_size > h or j + crop_size > w:\n",
    "                continue\n",
    "            patch_np = x[i:i+crop_size, j:j+crop_size]\n",
    "            patch = torch.from_numpy(patch_np[None]).permute(0,3,1,2).to(device).float()\n",
    "            quant, diff, info  = model.encode(patch)\n",
    "            _, _, z_index = info # z_index 16x16 = [256]\n",
    "            z_index = z_index.detach().cpu().numpy().reshape(16,16)\n",
    "            index_grid[i//16:i//16 + grid_size, j//16:j//16 + grid_size] = z_index\n",
    "    index_grid = index_grid[:i//16 + grid_size,:j//16+ grid_size]\n",
    "    return index_grid\n",
    "\n",
    "ids = [f[:-4].split('/')[-1] for f in glob.glob(os.path.join(dataroot, \"*.JPG\"))] # self.json_data[\"images\"]     \n",
    "crop_size = config.data.params.train.params.crop_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    for id in tqdm(ids):\n",
    "        image_path = os.path.join(dataroot, id + '.JPG')\n",
    "        segmentation_path = os.path.join(dataroot, id + '.npy')    \n",
    "        img, seg = preprocess_image(image_path, segmentation_path)\n",
    "        index_img = patch_encode(img, vggan)\n",
    "        index_seg = patch_encode(seg, vggan_cond)\n",
    "        \n",
    "        # save index matrix to patch\n",
    "        np.save(os.path.join(dataroot, datasetname, f\"{id}_img.npy\"), index_img)\n",
    "        np.save(os.path.join(dataroot, datasetname, f\"{id}_cond.npy\"), index_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = torch.from_numpy(patch_np[None]).permute(0,3,1,2).float()\n",
    "quant, diff, info  = vggan.encode(patch)\n",
    "_, _, z_index = info # z_index 256x256/(16x16) = [256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DJI_0838'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
