model:
  base_learning_rate: 4.5e-06
  target: taming.models.cond_transformer.Net2NetTransformer
  params:
    cond_stage_key: segmentation
    transformer_config:
      target: taming.modules.transformer.mingpt.CondGPT
      params:
        vocab_size: 8192
        vocab_size_cond: 1024
        block_size: 512  # = 256 + 256 = dim(vqgan_latent_space,16x16) + dim(conditional_builder.embedding_dim)
        n_layer: 40
        n_head: 16
        n_embd: 1408
        embd_pdrop: 0.1
        resid_pdrop: 0.1
        attn_pdrop: 0.1
    first_stage_config:
      target: taming.models.dummy_cond_stage.DummyCondStage
    cond_stage_config:
      target: taming.models.dummy_cond_stage.DummyCondStage

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 2
    train:
      target: taming.data.owt.OWTToken
      params:
        dataroot: "/home/ICT2000/chenh/Datasets/USC_Galen_Center"
        dataset_name: "tokenized_2048"
        crop_size: 16 # 256 / 16 = 16

disable_image_logging: True