{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (EXAMING VQGAN FIRST STEP MODEL)\n",
    "\n",
    "import torch\n",
    "\n",
    "# device = torch.cuda.device(0)\n",
    "device = torch.device('cuda:0')\n",
    "# eval testing\n",
    "\n",
    "# load config for transformer\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "config_path = \"configs/owt_vqgan.yaml\"\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "ckpt_path = f\"logs/allusc_vqgan_1024_baseline/checkpoints/last.ckpt\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "import yaml\n",
    "import torch.nn.functional as F\n",
    "# print(yaml.dump(OmegaConf.to_container(config)))\n",
    "config ['data']['params']['batch_size'] = 1\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# config['data']['params']['train']['params']['dataroot'] = \"/home/ICT2000/chenh/Haiwei/Datasets/OWT/USC_Galen_Center\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "unconditional = config.model.params.cond_stage_config == \"__is_unconditional__\"\n",
    "\n",
    "from taming.models.vqgan import VQModel\n",
    "model = VQModel(**config.model.params).to(device)\n",
    "\n",
    "# loading checkpoint\n",
    "import torch\n",
    "sd = torch.load(ckpt_path, map_location=device)[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "seg_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd06af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset instantiation\n",
    "import importlib\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "data = instantiate_from_config(config.data)\n",
    "data.prepare_data()\n",
    "data.setup()\n",
    "dataset = data.datasets['train']\n",
    "dataset_iter = iter(data._train_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ef039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "idx = 14\n",
    "batch = dataset[idx]\n",
    "print(batch.keys())\n",
    "\n",
    "tensify = lambda x: torch.from_numpy(x[None]).to(device).permute(0,3,1,2).contiguous().float()\n",
    "tensor_to_numpy = lambda x:x.detach().cpu().numpy()[0].transpose(1,2,0)\n",
    "\n",
    "def write_images(path, image, n_row=1):\n",
    "    image = ((image + 1) * 255 / 2).astype(np.uint8)\n",
    "    if image.ndim == 3:\n",
    "        if image.shape[2] == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGRA)\n",
    "    cv2.imwrite('{}'.format(str(path)), np.squeeze(image))\n",
    "\n",
    "def show_image(s, iftorch=False):\n",
    "  if iftorch:\n",
    "    s = s.detach().cpu().numpy()[0]\n",
    "  s = (1 + np.clip(s, -1, 1)) / 2 # (s - s.min()) / (s.max() - s.min())\n",
    "  s = (s * 255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "    \n",
    "# segmentation = batch['segmentation']\n",
    "# show_image(segmentation)\n",
    "\n",
    "image = batch['image']\n",
    "show_image(image)\n",
    "# write_images('test.png',image)\n",
    "# write_images('test_seg.png',segmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: tokenization with regard to inpainting\n",
    "\n",
    "\n",
    "def CentralMask(s, offset=None):\n",
    "    mask = np.ones((s, s), np.uint8)\n",
    "    if offset is None:\n",
    "        offset = s // 4\n",
    "    else:\n",
    "        offset = max(min(offset, 0), s//2 - 1)\n",
    "    mask[offset:offset+s//2, offset:offset+s//2] = 0\n",
    "    return mask[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "# first tokenize the original input\n",
    "x = tensify(image) \n",
    "quant_z, _, info = model.encode(x)\n",
    "_, _, indices = info\n",
    "\n",
    "# then masked out central area of the image and tokenize it again\n",
    "x_masked = tensify(image * CentralMask(image.shape[0]))\n",
    "quant_z_masked, _, info_masked = model.encode(x_masked)\n",
    "_, _, indices_masked = info_masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e38423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct with the tokens\n",
    "img_recon = model.decode(quant_z)\n",
    "show_image(tensor_to_numpy(img_recon), False)\n",
    "\n",
    "# reconstructed masked image\n",
    "img_mask_recon = model.decode(quant_z_masked)\n",
    "show_image(tensor_to_numpy(img_mask_recon) * CentralMask(image.shape[0]), False)\n",
    "\n",
    "# masked image\n",
    "show_image(image * CentralMask(image.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_index_grid(grid, horizontal_bar=False):\n",
    "    for row in grid:\n",
    "        row_str = \"\"\n",
    "        for number in row:\n",
    "            row_str += '{0:<5}|'.format(number)\n",
    "        if horizontal_bar:\n",
    "            hori = \"\"\n",
    "            for i in range(len(row_str)):\n",
    "                hori += '_'\n",
    "            print(hori)\n",
    "        print(row_str)\n",
    "\n",
    "indices_masked_np = indices_masked.reshape(16,16).detach().cpu().numpy()\n",
    "indices_np = indices.reshape(16,16).detach().cpu().numpy()\n",
    "\n",
    "print_index_grid(indices_np)\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print_index_grid(indices_masked_np)\n",
    "\n",
    "state_grid = list()\n",
    "\n",
    "sim_grid = indices_np == indices_masked_np\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print_index_grid(sim_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca90718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize some params\n",
    "mask = 1.0\n",
    "mask_tensor = None\n",
    "nb = 1 #c_code.shape[0]\n",
    "codebook_size = config.model.params.first_stage_config.params.embed_dim\n",
    "# z_indices_shape = c_indices.shape\n",
    "c_code_res = 16\n",
    "res = 256\n",
    "z_indices_shape = [nb,res]\n",
    "z_code_shape = [nb, codebook_size,c_code_res,c_code_res]\n",
    "print(z_code_shape, z_indices_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb933f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (caution! slow execution) (optional) batch image of the original size\n",
    "image_path = dataset[idx]['img_path']\n",
    "full_image = Image.open(image_path)\n",
    "full_img = np.array(full_image).astype(np.float32) \n",
    "full_img = (img/127.5 - 1.0)\n",
    "show_image(full_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f501fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) sample random indices to image VQQGAN\n",
    "# z_code_shape = codebook_size\n",
    "z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)\n",
    "print(z_indices.shape)\n",
    "x_sample = model.decode_to_img(z_indices, z_code_shape)\n",
    "print(x_sample.shape)\n",
    "if seg_model:\n",
    "    x_sample = F.softmax(x_sample,dim=1)\n",
    "show_image(tensor_to_numpy(x_sample), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9899c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) sample random indices to image VQQGAN\n",
    "@torch.no_grad()\n",
    "def decode_to_img(model, index, zshape, use_softmax=False):\n",
    "    bhwc = (zshape[0],zshape[2],zshape[3],zshape[1])\n",
    "    quant_z = model.quantize.get_codebook_entry(\n",
    "        index.reshape(-1), shape=bhwc)\n",
    "    x = model.decode(quant_z)\n",
    "    return x\n",
    "\n",
    "codebook_size = config.model.params.embed_dim\n",
    "c_code_res = 8\n",
    "res = 128\n",
    "z_indices_shape = [nb,c_code_res**2]\n",
    "z_code_shape = [nb, codebook_size,c_code_res,c_code_res]\n",
    "z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)\n",
    "x_sample = decode_to_img(model, z_indices, z_code_shape)\n",
    "print(z_indices.shape)\n",
    "show_image(tensor_to_numpy(x_sample), False)\n",
    "\n",
    "#(optional VQGAN testing)\n",
    "x = tensify(batch['image']) \n",
    "quant_z, _, info = model.encode(x)\n",
    "indices = info[2].view(quant_z.shape[0], -1)\n",
    "x_sample = decode_to_img(model, indices, z_code_shape)\n",
    "show_image(tensor_to_numpy(x_sample), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS IF NO SEGMENTATION\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "idx = 5\n",
    "batch = dataset[idx]\n",
    "print(batch.keys())\n",
    "\n",
    "tensify = lambda x: torch.from_numpy(x[None]).to(device).permute(0,3,1,2).contiguous().float()\n",
    "tensor_to_numpy = lambda x:x.detach().cpu().numpy()[0].transpose(1,2,0)\n",
    "\n",
    "def write_images(path, image, n_row=1):\n",
    "    image = ((image + 1) * 255 / 2).astype(np.uint8)\n",
    "    if image.ndim == 3:\n",
    "        if image.shape[2] == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGRA)\n",
    "    cv2.imwrite('{}'.format(str(path)), np.squeeze(image))\n",
    "\n",
    "def show_image(s, iftorch=False):\n",
    "  if iftorch:\n",
    "    s = s.detach().cpu().numpy()[0]\n",
    "  s = ((s + 1) * 255 / 2).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "    \n",
    "image = batch['image']\n",
    "show_image(image)\n",
    "\n",
    "# initialize some params\n",
    "mask = 1.0\n",
    "mask_tensor = None\n",
    "nb = 1 #c_code.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder structure testing\n",
    "device = feat.device\n",
    "weight = torch.rand(256, 128, 2, 2).float()\n",
    "mapping_conv = torch.nn.Conv2d(256, 128, kernel_size=5, stride=1, padding=2)\n",
    "feat = quant_z # nb, nc, h, w\n",
    "mapping_conv = mapping_conv.to(device)\n",
    "print(feat.shape)\n",
    "mapping_conv(feat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 16\n",
    "for i, i_level in enumerate(reversed(range(4))):\n",
    "    print(i_level, i)\n",
    "    res *= 2\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d87c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestrictedUpconv2D(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=2, activation='relu'):\n",
    "        super().__init__()\n",
    "        if isinstance(stride, tuple):\n",
    "            assert len(stride) == 2\n",
    "            sx, sy = stride\n",
    "        else:\n",
    "            assert isinstance(stride, int)\n",
    "            sx = sy = stride\n",
    "        self.mlp = torch.nn.Conv2d(c_in, c_out*sx*sy, kernel_size=1, bias=False)\n",
    "        self.sx = sx\n",
    "        self.sy = sy\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.mlp(x) # B, C*sx*sy, H, W \n",
    "        x = x.reshape(B, -1, self.sx, self.sy, H, W)\n",
    "        x = x.permute(0,1,4,2,5,3).reshape(B, -1, self.sx*H, self.sy*W)\n",
    "        return x\n",
    "    \n",
    "feat_ones = torch.zeros_like(feat).to(device)\n",
    "feat_ones[0,0,1,4] = 1.0\n",
    "feat_ones[0,0,1,3] = 1.0\n",
    "feat_ones[0,0,3,5] = 1.0\n",
    "\n",
    "upconv = torch.nn.Sequential(LocalizedUpconv2D(256, 128).to(device),\n",
    "                             LocalizedUpconv2D(128, 64).to(device))\n",
    "upfeat = upconv(feat_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "featmap = upfeat[0,0].unsqueeze(-1).detach().cpu().numpy()\n",
    "featmap = (featmap - featmap.min()) / (featmap.max() - featmap.min())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(featmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional model or segmentation model\n",
    "if not unconditional:\n",
    "    # reconstructing seg tensor\n",
    "    seg_tensor = tensify(segmentation)\n",
    "    c_code, c_indices = model.encode_to_c(seg_tensor)\n",
    "    seg_rec = model.cond_stage_model.decode(c_code)\n",
    "    seg_rec = F.softmax(seg_rec,dim=1)\n",
    "    show_image(tensor_to_numpy(seg_rec), False)\n",
    "    \n",
    "if seg_model:\n",
    "    # reconstructing seg tensor\n",
    "    seg_tensor = tensify(segmentation)\n",
    "    c_code, c_indices = model.encode_to_z(seg_tensor)\n",
    "    seg_rec = model.first_stage_model.decode(c_code)\n",
    "    seg_rec = F.softmax(seg_rec,dim=1)\n",
    "    show_image(tensor_to_numpy(seg_rec), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpainting testing: replace random z indices with that of an encoded batch image\n",
    "key = 'segmentation' if seg_model else \"image\"\n",
    "x = tensify(batch[key]) \n",
    "if unconditional:\n",
    "    c = x\n",
    "    quant_c, c_indices = model.encode_to_c(c)\n",
    "else:\n",
    "    c = tensify(batch['segmentation'])\n",
    "    quant_c = c_code\n",
    "# c = x if unconditional else tensify(batch['segmentation'])\n",
    "quant_z, z_indices = model.encode_to_z(x)\n",
    "# quant_c, c_indices = model.encode_to_c(c)\n",
    "gt_z_indices = z_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) mlp testing\n",
    "\n",
    "import kornia\n",
    "import torch.distributions as dists\n",
    "\n",
    "def flip_positions(positions):\n",
    "    positions = positions.transpose(2,3)\n",
    "    positions = torch.cat([positions[:,0][:,None],-positions[:,1][:,None]],1)\n",
    "    return positions.contiguous()\n",
    "\n",
    "\n",
    "def get_position(size, dim, device, batch_size):\n",
    "    height, width = size\n",
    "    aspect_ratio = width / height\n",
    "    position = kornia.utils.create_meshgrid(width, height, device=torch.device(device)).permute(0, 3, 1, 2)\n",
    "    position[:, 1] = -position[:, 1] * aspect_ratio  # flip y axis\n",
    "\n",
    "    if dim == 1:\n",
    "        x, y = torch.split(position, 1, dim=1)\n",
    "        position = x\n",
    "    if dim == 3:\n",
    "        x, y = torch.split(position, 1, dim=1)\n",
    "        z = torch.ones_like(x) * torch.rand(1, device=device) * 2 - 1\n",
    "        a = torch.randint(0, 3, (1,)).item()\n",
    "        if a == 0:\n",
    "            xyz = [x, y, z]\n",
    "        elif a == 1:\n",
    "            xyz = [z, x, y]\n",
    "        else:\n",
    "            xyz = [x, z, y]\n",
    "        # xyz =  [x,z,y]\n",
    "        position = torch.cat(xyz, dim=1)\n",
    "    position = position.expand(batch_size, dim, width, height)\n",
    "    return position if dim == 3 else flip_positions(position)\n",
    "\n",
    "def get_distribution_type(shape, type='uniform'):\n",
    "    if type == 'normal':\n",
    "        return dists.Normal(torch.zeros(shape), torch.ones(shape))\n",
    "    elif type == 'uniform':\n",
    "        return dists.Uniform(torch.zeros(shape) - 1, torch.ones(shape))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_cropped_coord_grid(device, \n",
    "                           dist_shift, \n",
    "                           nb,  \n",
    "                           res=256, \n",
    "                           crop_res=128, \n",
    "                           scale=2.0):\n",
    "\n",
    "    coord_grid = scale * get_position([res, res], 2, device, nb)\n",
    "    # dist_shift = get_distribution_type([1,2], type='uniform')\n",
    "    coord_grid_sample = torch.zeros([nb, 2, crop_res, crop_res]).to(device)\n",
    "    shift = 0.5 * (dist_shift.sample([nb]) + 1) * (res - crop_res)\n",
    "    shift = shift.int().reshape(nb,2)\n",
    "    for idx, s in enumerate(shift):\n",
    "        dx,dy = s\n",
    "        dx = int(dx.item())\n",
    "        dy = int(dy.item())\n",
    "        coord_grid_sample[idx] = coord_grid[idx, :, dx:dx+crop_res, dy:dy+crop_res]\n",
    "    return coord_grid_sample, shift\n",
    "\n",
    "def crop_input(device, x, shift, nb, crop_res=128):\n",
    "    new_x = torch.zeros([nb, 3, crop_res, crop_res]).to(device)\n",
    "    for idx, s in enumerate(shift):\n",
    "        dx, dy = s\n",
    "        dx = int(dx.item())\n",
    "        dy = int(dy.item())       \n",
    "        new_x[idx] = x[idx,:,dx:dx+crop_res,dy:dy+crop_res]\n",
    "    return new_x\n",
    "\n",
    "def batched_index_select_2d(index, feature):\n",
    "    batch_size, image_dim, res1, res2 = index.shape\n",
    "    _, f_dim, grid_dim, _ = feature.shape\n",
    "    indexf = index.reshape(batch_size,image_dim,-1)\n",
    "    indexf2 = indexf[:,0] * grid_dim + indexf[:,1]\n",
    "    def select(x, idx):\n",
    "        x_select = torch.index_select(x.reshape(f_dim, -1),1,idx)\n",
    "        x_select = x_select.reshape(f_dim, res1, res2)\n",
    "        return x_select[None]\n",
    "    xyz_select = torch.cat([select(x,idx) for x,idx in zip(feature, indexf2)], dim=0)\n",
    "    return xyz_select\n",
    "\n",
    "def stationary_noise(positions, feature, scale=2.0, sigma=0.2, mode='gaussian'):\n",
    "    '''\n",
    "    positions: nb,2,w,h\n",
    "    codebook feature: nb,nc,c_w,c_h (assume to be defining the feature space in [-scale, scale])\n",
    "    '''\n",
    "    # cgs_q: coordinate_grid_sampled_quantized (according to codebook resolution)\n",
    "    c_res = feature.shape[2]\n",
    "    cgs_q = (positions + scale) * c_res / (2 * scale)\n",
    "    \n",
    "    # index-select from features    \n",
    "    idx_1 = torch.clamp(torch.floor(cgs_q).long(), 0, c_res - 1)\n",
    "    idx_2 = torch.clamp(idx_1 + 1, 0, c_res - 1)\n",
    "    idx_3 = torch.clamp(torch.cat([idx_1[:,0].unsqueeze(1) + 1, idx_1[:,1].unsqueeze(1)],1), 0, c_res-1)\n",
    "    idx_4 = torch.clamp(torch.cat([idx_1[:,0].unsqueeze(1), idx_1[:,1].unsqueeze(1)+1],1), 0, c_res-1)\n",
    "\n",
    "    # distance to corners\n",
    "    px = cgs_q[:,0]\n",
    "    py = cgs_q[:,1]\n",
    "    x1 = (px - torch.floor(px))\n",
    "    x2 = (torch.floor(px) + 1 - px)\n",
    "    y1 = (py - torch.floor(py))\n",
    "    y2 = (torch.floor(py) + 1 - py)  \n",
    "    \n",
    "    if mode == 'gaussian':\n",
    "        f_grouped = torch.cat([batched_index_select_2d(index, feature)[...,None] \\\n",
    "                    for index in [idx_1,idx_2,idx_3,idx_4]],dim=-1)\n",
    "        dist1 = torch.sqrt(x1**2 + y1**2)[...,None]\n",
    "        dist2 = torch.sqrt(x2**2 + y2**2)[...,None]\n",
    "        dist3 = torch.sqrt(x2**2 + y1**2)[...,None]\n",
    "        dist4 = torch.sqrt(x1**2 + y2**2)[...,None]\n",
    "        dists = torch.cat([dist1,dist2,dist3,dist4], 3)\n",
    "        dists = torch.nn.functional.softmax(-dists/sigma, -1).unsqueeze(1)\n",
    "        return torch.sum(dists * f_grouped, -1)\n",
    "    elif mode == 'bilinear' or mode == 'linear':\n",
    "        tr,tl,br,bl = [batched_index_select_2d(index, feature)\\\n",
    "                    for index in [index_1,index_3,index_4,index_2]]\n",
    "        bx = x1.unsqueeze(1) * bl + x2.unsqueeze(1) * br\n",
    "        tx = x1.unsqueeze(1) * tl + x2.unsqueeze(1) * tr        \n",
    "        return y1.unsqueeze(1) * bx + y2.unsqueeze(1) * tx        \n",
    "    else:\n",
    "        raise NotImplementedError(f\"type of interpolation {mode} is not recognized!\")\n",
    "\n",
    "def disabled_train(self, mode=True):\n",
    "    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n",
    "    does not change anymore.\"\"\"\n",
    "    return self\n",
    "\n",
    "# x -> [cos(2^0 pi x), ..., sin(2^9) pi x]\n",
    "# b,dim,... -> b,2*dim*l,...\n",
    "def positional_encoding(x, l=5, beta=None):\n",
    "    bs,dim = x.shape[:2]\n",
    "    res = x.shape[2:]\n",
    "    x = x.unsqueeze(2).expand(bs,dim,l,*res)\n",
    "    octaves = 2**(torch.arange(l)).to(x.device)\n",
    "    if beta is not None:        \n",
    "        octaves = octaves * beta\n",
    "    for r in res:\n",
    "        octaves = octaves.unsqueeze(-1)\n",
    "    x = x * octaves[None,None,...] * np.pi\n",
    "    x = torch.cat((torch.sin(x).unsqueeze(2), torch.cos(x).unsqueeze(2)),2)\n",
    "    return x.reshape(bs,-1,*res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_z.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81613a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing features\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def tensor_to_visual(img, normalize=False):\n",
    "    signed_to_unsigned = lambda x : (x + 1) / 2\n",
    "    img = img[0].detach().cpu().numpy()\n",
    "    if img.min() < 0:\n",
    "        img = signed_to_unsigned(img)\n",
    "    # img = (img - img.min()) / (img.max() - img.min())    \n",
    "    if normalize:\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-4)\n",
    "    img = np.clip(img,0.0,1.0)\n",
    "    return img.transpose(1,2,0)\n",
    "\n",
    "z_original = F.interpolate(quant_z, [256, 256])\n",
    "z_img = tensor_to_visual(z_original[:,:3], True)\n",
    "plt.imshow(z_img, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_shift = get_distribution_type([1,2], type='uniform')\n",
    "scale = 2.0\n",
    "# # random cropping to model stationary shift\n",
    "cgs, shift = get_cropped_coord_grid(quant_z.device, \n",
    "                                    dist_shift, \n",
    "                                    quant_z.shape[0], \n",
    "                                    x.shape[2], \n",
    "                                    128, \n",
    "                                    scale)\n",
    "cropped_x = crop_input(quant_z.device, x, shift, quant_z.shape[0], crop_res=128)\n",
    "cgs = scale * get_position([256, 256], 2, x.device, x.shape[0])\n",
    "#feature sampling\n",
    "feat = stationary_noise(cgs, quant_z, scale=scale)  \n",
    "feat_vis = tensor_to_visual(feat[:,:3], True)\n",
    "plt.imshow(feat_vis, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc33d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize some other params related to transformer\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import os\n",
    "\n",
    "output_dir = os.path.join(\"logs\", \"eval\", \"notebook_vis\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "idx = z_indices\n",
    "idx = idx.reshape(z_code_shape[0],z_code_shape[2],z_code_shape[3])\n",
    "\n",
    "if not unconditional:\n",
    "    cidx = c_indices\n",
    "    cidx = cidx.reshape(c_code.shape[0],c_code.shape[2],c_code.shape[3])\n",
    "\n",
    "temperature = 1.0\n",
    "top_k = 100\n",
    "update_every = 10\n",
    "\n",
    "nx = z_code_shape[2]\n",
    "ny = z_code_shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpainting testing: replace random z indices with that of an encoded batch image\n",
    "\n",
    "MASK_LABEL = 1\n",
    "\n",
    "\n",
    "key = 'segmentation' if seg_model else \"image\"\n",
    "x = tensify(batch[key]) \n",
    "if unconditional:\n",
    "    c = x\n",
    "    quant_c, c_indices = model.encode_to_c(c)\n",
    "else:\n",
    "    c = tensify(batch['segmentation'])\n",
    "    quant_c = c_code\n",
    "# c = x if unconditional else tensify(batch['segmentation'])\n",
    "quant_z, z_indices = model.encode_to_z(x)\n",
    "# quant_c, c_indices = model.encode_to_c(c)\n",
    "gt_z_indices = z_indices\n",
    "\n",
    "# inpainting demo: masking out a certain region of the image\n",
    "seg_label = np.argmax(segmentation, axis=2)\n",
    "mask = (seg_label != MASK_LABEL).astype(np.float32)[...,None]\n",
    "show_image(segmentation * mask)\n",
    "show_image(image * mask)\n",
    "\n",
    "# idx, cidx: codebook of the encoded image\n",
    "mask_tensor = torch.from_numpy(mask).to(idx.device).float()\n",
    "mask_tensor = mask_tensor.unsqueeze(0)[...,0]\n",
    "mask_tensor = torch.nn.functional.interpolate(mask_tensor.unsqueeze(1), scale_factor=1/16)\n",
    "mask_tensor = mask_tensor[:,0].int()\n",
    "# print(mask_tensor.reshape(-1))\n",
    "# print(mask_tensor.shape, idx.shape)\n",
    "\n",
    "# mask idx \n",
    "idx = idx * mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b55a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a multiplied-scale scene with a large (partially fixed) random codebook\n",
    "multiplier = 1\n",
    "nnx = nx * multiplier\n",
    "nny = ny * multiplier\n",
    "c_code_res = 16\n",
    "step_size = 8\n",
    "no_mask = False\n",
    "det = False\n",
    "\n",
    "target_image = np.zeros([multiplier*256, multiplier*256, 3])\n",
    "z_indices = torch.randint(codebook_size, [nb,(multiplier**2)*res], device=model.device)\n",
    "z_indices = z_indices.reshape(nb, nnx, nny)\n",
    "\n",
    "# (demo) masking gt_z_indices\n",
    "if mask_tensor is not None:\n",
    "    input_z_indices = gt_z_indices * mask_tensor.reshape(1, -1)\n",
    "else:\n",
    "    input_z_indices = gt_z_indices\n",
    "    \n",
    "# partially fill the z_indices with true data\n",
    "z_indices[:,:nx, :ny] = input_z_indices.reshape(nb, nx, ny)\n",
    "occupancy = np.zeros(z_indices.shape).astype(bool)\n",
    "\n",
    "if no_mask:\n",
    "    pass\n",
    "elif mask_tensor is not None:\n",
    "    occupancy[:,:nx, :ny] = mask_tensor.bool().detach().cpu().numpy()\n",
    "else:\n",
    "    occupancy[:,:nx, :ny] = True\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "# outer loop: image to decode\n",
    "for i in range(0, nnx - step_size, step_size):\n",
    "    for j in range(0, nny - step_size, step_size):\n",
    "        idx = z_indices[:, i:i+c_code_res, j:j+c_code_res].reshape(nb, -1)\n",
    "        occ = occupancy[:, i:i+c_code_res, j:j+c_code_res].reshape(nb, -1)       \n",
    "        # only update a block in idx if it is not occupied\n",
    "        for ii in range(idx.shape[1]):\n",
    "            if not occ[0,ii]:\n",
    "                if unconditional:\n",
    "                    patch = torch.cat((c_indices, idx[:,:ii]), dim=1)\n",
    "                    logits,_ = model.transformer(patch)\n",
    "                else:\n",
    "                    cpatch = cidx[:, i:i+c_code_res, j:j+c_code_res].reshape(nb, -1)\n",
    "                    # cpatch = cidx.reshape(nb, -1)\n",
    "                    logits,_ = model.transformer(idx[:,:ii], cpatch)\n",
    "                # patch = torch.cat((c_indices, idx[:,:ii]),1)\n",
    "                # logits,_ = model.transformer(patch)\n",
    "                logits = logits[:, -1, :]\n",
    "                logits = logits/temperature\n",
    "                if top_k is not None:\n",
    "                  logits = model.top_k_logits(logits, top_k)\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                if det:\n",
    "                    _, idx[:,ii] = torch.topk(probs, k=1, dim=-1)\n",
    "                else:\n",
    "                    idx[:,ii] = torch.multinomial(probs, num_samples=1)\n",
    "                occ[:,ii] = True\n",
    "        idx = idx.reshape(nb, c_code_res, c_code_res)\n",
    "        occ = occ.reshape(nb, c_code_res, c_code_res)\n",
    "        z_indices[:, i:i+c_code_res, j:j+c_code_res] = idx\n",
    "        occupancy[:, i:i+c_code_res, j:j+c_code_res] = occ\n",
    "        print(f\"Time: {time.time() - start_t} seconds\")\n",
    "        print(f\"Step: ({i},{j})\")\n",
    "\n",
    "new_z_code_shape = [nb, codebook_size, multiplier*16, multiplier*16]\n",
    "x_sample = model.decode_to_img(z_indices, new_z_code_shape)\n",
    "if seg_model:\n",
    "    x_sample = F.softmax(x_sample, dim=1)\n",
    "target_image = x_sample[0].detach().cpu().numpy().transpose(1,2,0)\n",
    "show_image(segmentation)\n",
    "show_image(image * mask)\n",
    "show_image(target_image, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, nnx, c_code_res):\n",
    "    for j in range(0, nny, c_code_res):\n",
    "        patch_code = z_indices[:, i:i+c_code_res, j:j+c_code_res]\n",
    "        x_sample = model.decode_to_img(patch_code, z_code_shape)\n",
    "        if seg_model:\n",
    "            x_sample = F.softmax(x_sample, dim=1)\n",
    "        patch_image = x_sample[0].detach().cpu().numpy().transpose(1,2,0)\n",
    "        target_image[i*16:i*16+256, j*16:j*16+256] = patch_image\n",
    "show_image(target_image, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0efb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the original scene with a known encoded codebook\n",
    "update_every = 1000000000\n",
    "multiplier = 1\n",
    "quant_z, z_indices = model.encode_to_z(x)\n",
    "idx = z_indices\n",
    "idx = idx.reshape(z_code_shape[0], z_code_shape[2],z_code_shape[3])\n",
    "\n",
    "nx = z_code_shape[2]\n",
    "ny = z_code_shape[3]\n",
    "start_t = time.time()\n",
    "\n",
    "for i in range(0, nx-0):\n",
    "  if i <= 8:\n",
    "    local_i = i\n",
    "  elif nx-i < 8:\n",
    "    local_i = 16-(nx-i)\n",
    "  else:\n",
    "    local_i = 8\n",
    "  for j in range(0,ny-0):\n",
    "    if j <= 8:\n",
    "      local_j = j\n",
    "    elif ny-j < 8:\n",
    "      local_j = 16-(ny-j)\n",
    "    else:\n",
    "      local_j = 8\n",
    "    \n",
    "    i_start = i-local_i\n",
    "    i_end = i_start+16\n",
    "    j_start = j-local_j\n",
    "    j_end = j_start+16\n",
    "    # print(i_start, i_end, j_start, j_end)\n",
    "    patch = idx[:,i_start:i_end,j_start:j_end]\n",
    "    patch = patch.reshape(patch.shape[0],-1)\n",
    "    if unconditional:\n",
    "        patch = torch.cat((c_indices, patch), dim=1)\n",
    "        logits,_ = model.transformer(patch[:,:-1])\n",
    "    else:\n",
    "        cpatch = cidx[:, i_start:i_end, j_start:j_end]\n",
    "        cpatch = cpatch.reshape(cpatch.shape[0], -1)\n",
    "        # patch = torch.cat((cpatch, patch), dim=1)\n",
    "        logits,_ = model.transformer(patch[:,:-1], cpatch)\n",
    "\n",
    "    logits = logits[:, -256:, :]\n",
    "    logits = logits.reshape(z_code_shape[0],16,16,-1)\n",
    "    logits = logits[:,local_i,local_j,:]\n",
    "    logits = logits/temperature\n",
    "    \n",
    "    if top_k is not None:\n",
    "      logits = model.top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    idx[:,i,j] = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    step = i*z_code_shape[3]+j\n",
    "    if step%update_every==0 or step==z_code_shape[2]*z_code_shape[3]-1:\n",
    "      x_sample = model.decode_to_img(idx, z_code_shape)\n",
    "      clear_output()\n",
    "      print(f\"Time: {time.time() - start_t} seconds\")\n",
    "      print(f\"Step: ({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})\")\n",
    "      print(x_sample.shape)\n",
    "      if not unconditional:\n",
    "          show_image(tensor_to_numpy(seg_rec), False)\n",
    "      show_image(tensor_to_numpy(x_sample), False)\n",
    "      # write_images(os.path.join(output_dir, f\"step{step}_{i}_{j}.png\"), tensor_to_numpy(x_sample))\n",
    "      patch_output = tensor_to_numpy(x_sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
