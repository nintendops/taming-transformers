{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import importlib\n",
    "import yaml\n",
    "import albumentations\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imsave\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.notebook import tqdm\n",
    "from taming.modules.util import box_mask, BatchRandomMask\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "def imshow(images, titles=None):\n",
    "    n_img = len(images)\n",
    "    plt.rcParams['figure.figsize'] = [8*n_img, 8*n_img]\n",
    "    \n",
    "    if n_img > 1:\n",
    "        fig, ax = plt.subplots(1, n_img)\n",
    "        for i in range(n_img):\n",
    "            if titles is not None and i < len(titles):\n",
    "                ax[i].set_title(titles[i])\n",
    "            ax[i].axis('off')\n",
    "            ax[i].imshow(images[i])\n",
    "    else:\n",
    "        if titles is not None:\n",
    "            plt.set_titile(titles[0])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[0])\n",
    "\n",
    "def center_crop(image, s=512):\n",
    "    h, w = image.shape[:2]\n",
    "    assert s <= h and s <= w\n",
    "    ih = (h - s) // 2\n",
    "    iw = (w - s) // 2\n",
    "    return image[ih:ih+s, iw:iw+s]\n",
    "\n",
    "# load a single input\n",
    "def preprocess(x, res=512, normalize=True):\n",
    "    if normalize:\n",
    "        x = x.transpose(2,0,1)\n",
    "        x = (torch.from_numpy(x).float().to(device) / 127.5 - 1).unsqueeze(0)\n",
    "    else:\n",
    "        x = torch.from_numpy(x).float().to(device).unsqueeze(0)\n",
    "    return torch.nn.functional.interpolate(x, size=(res,res))\n",
    "    \n",
    "def to_img(x):\n",
    "    x = (x.permute(0, 2, 3, 1) * 127.5 + 127.5).round().clamp(0, 255).to(torch.uint8)\n",
    "    return x[0].detach().cpu().numpy()\n",
    "\n",
    "def readmask(path):\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "    return mask\n",
    "\n",
    "# validation input\n",
    "# places_val_files = glob.glob(\"/home/ICT2000/chenh/Haiwei/Datasets/Places365/val_large/*.jpg\")\n",
    "places_val_files = glob.glob(\"/home/chenh/data/test_large/*.jpg\")\n",
    "\n",
    "# images = [center_crop(imread(f)) for f in places_val_files[k:k+3]]\n",
    "# imshow(images)\n",
    "\n",
    "# validation results (MAT)\n",
    "places_val_files.sort()\n",
    "image_names = [os.path.basename(p).replace('jpg', 'png') for p in places_val_files]\n",
    "MAT_rstfolder = \"/home/ICT2000/chenh/Haiwei/MAT/pr6_places_final\"\n",
    "\n",
    "# preds = [imread(os.path.join(MAT_rstfolder, n)) for n in image_names]\n",
    "# imshow(preds)\n",
    "\n",
    "# mask folder\n",
    "# maskfolder = \"/home/ICT2000/chenh/Haiwei/Datasets/Places365/masks_val_512_eval\"\n",
    "ids = [im[-10:-4] for im in image_names]\n",
    "\n",
    "# masks = [readmask(os.path.join(maskfolder, f\"{id}.png\")) for id in ids]\n",
    "# imshow(masks)\n",
    "\n",
    "def get_data(k, res):\n",
    "    gt = preprocess(center_crop(imread(places_val_files[k])), res)\n",
    "    try:\n",
    "        mask_in = preprocess(readmask(os.path.join(maskfolder, f\"{ids[k]}.png\"))[None], res,normalize=False)\n",
    "    except Exception:\n",
    "        return gt, None\n",
    "    return gt, mask_in\n",
    "\n",
    "current_dir = os.path.abspath('.')\n",
    "current_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d55a2-a311-4feb-871d-0b769ef593ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(current_dir)\n",
    "\n",
    "# Test dataloader (optional)\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "# load config for transformer\n",
    "config_path = \"configs/places_decoder.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "config['data']['params']['batch_size'] = 1\n",
    "config['data']['params']['test']['params']['crop_size'] = 512\n",
    "config['data']['params']['test']['params']['rescale_size'] = 512\n",
    "\n",
    "data = instantiate_from_config(config.data)\n",
    "data.prepare_data()\n",
    "data.setup()\n",
    "dataset = data.datasets['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fe2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/home/ICT2000/chenh/Haiwei/Datasets/Places365/test_large/Places365_test_00195819.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MAT network models\n",
    "MAT_folder = \"/home/ICT2000/chenh/Haiwei/MAT/MAT\"\n",
    "os.chdir(MAT_folder)\n",
    "from networks.mat import Generator\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "ckpt_path = \"models/Places_512.pkl\" # Places_512_FullData.pkl\n",
    "device = torch.device('cuda:1')\n",
    "with dnnlib.util.open_url(ckpt_path) as f:\n",
    "    G_saved = legacy.load_network_pkl(f)['G_ema'].to(device).eval().requires_grad_(False) # type: ignore\n",
    "\n",
    "def named_params_and_buffers(module):\n",
    "    assert isinstance(module, torch.nn.Module)\n",
    "    return list(module.named_parameters()) + list(module.named_buffers())\n",
    "\n",
    "\n",
    "def copy_params_and_buffers(src_module, dst_module, require_all=False):\n",
    "    assert isinstance(src_module, torch.nn.Module)\n",
    "    assert isinstance(dst_module, torch.nn.Module)\n",
    "    src_tensors = {name: tensor for name, tensor in named_params_and_buffers(src_module)}\n",
    "    for name, tensor in named_params_and_buffers(dst_module):\n",
    "        assert (name in src_tensors) or (not require_all)\n",
    "        if name in src_tensors:\n",
    "            tensor.copy_(src_tensors[name].detach()).requires_grad_(tensor.requires_grad)\n",
    "\n",
    "net_res = 512\n",
    "G = Generator(z_dim=512, c_dim=0, w_dim=512, img_resolution=net_res, img_channels=3).to(device).eval().requires_grad_(False)\n",
    "copy_params_and_buffers(G_saved, G, require_all=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with the MAT pretrained model\n",
    "with torch.no_grad():\n",
    "    k = 42\n",
    "    res = 512\n",
    "    \n",
    "    gt = preprocess(center_crop(imread(places_val_files[k])), res)\n",
    "    # gt = preprocess(center_crop(imread(test_path)), res)\n",
    "    \n",
    "    # batch = dataset[16]\n",
    "    # gt = torch.from_numpy(batch['image'][None]).to(device)\n",
    "    # gt = gt.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format)\n",
    "\n",
    "    x = gt\n",
    "    \n",
    "    mask_in = preprocess(readmask(os.path.join(maskfolder, f\"{ids[k]}.png\"))[None], res,normalize=False)\n",
    "\n",
    "    #############################\n",
    "    mask = box_mask(x.shape, x.device, 0.8, det=True).float()\n",
    "    mask_in = torch.round(mask).to(device)\n",
    "    ##################################\n",
    "    \n",
    "    # pred_ref = preprocess(imread(os.path.join(MAT_rstfolder, image_names[k])), res)\n",
    "    x_in = gt * mask_in\n",
    "\n",
    "    # inference with MAT network models\n",
    "\n",
    "    # no Labels.\n",
    "    label = torch.zeros([1, G.c_dim], device=device)\n",
    "    z = torch.from_numpy(np.random.randn(1, G.z_dim)).to(device)\n",
    "    output = G(x_in, mask_in, z, label, truncation_psi=1, noise_mode='const')\n",
    "    output = (output.permute(0, 2, 3, 1) * 127.5 + 127.5).round().clamp(0, 255).to(torch.uint8)\n",
    "    output_mat = output[0].detach().cpu().numpy()\n",
    "\n",
    "imshow([to_img(x_in), output_mat], titles=['input','pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f854e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our model\n",
    "os.chdir(current_dir)\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "config_path = \"configs/places_inpainting.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "config['data']['params']['batch_size'] = 1\n",
    "\n",
    "config['model']['params']['encoder_config']['params']['ddconfig']['clamp_ratio'] = 0.25\n",
    "\n",
    "model = instantiate_from_config(config.model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: using dataloader \n",
    "data = instantiate_from_config(config.data)\n",
    "data.prepare_data()\n",
    "data.setup()\n",
    "dataset = data.datasets['test']\n",
    "batch = dataset[0]\n",
    "batch['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39449184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.modules.util import scatter_mask, box_mask, mixed_mask, BatchRandomMask\n",
    "\n",
    "# examplar: 41\n",
    "\n",
    "# 4843 (dec), 2421 (dec), 717 (git), 2376 (git), 37 (dec)\n",
    "\n",
    "# 28 (dec), 34 (git)\n",
    "\n",
    "# batch = dataset[0]\n",
    "# print(batch['filename_'])\n",
    "\n",
    "def forward_to_indices(model, batch, z_indices, mask):\n",
    "    x, c = model.get_xc(batch)\n",
    "    x = x.to(device=device).float()\n",
    "    c = c.to(device=device).float()\n",
    "    quant_c, c_indices = model.encode_to_c(c)\n",
    "    mask = model.preprocess_mask(mask, z_indices)\n",
    "    r_indices = torch.full_like(z_indices, model.mask_token)\n",
    "    z_start_indices = mask*z_indices+(1-mask)*r_indices      \n",
    "    index_sample, probs, candidates = model.sample(z_start_indices.to(device=device), \n",
    "                               c_indices.to(device=device),\n",
    "                               sampling_ratio=0.2,\n",
    "                               temperature = 1.0,\n",
    "                               sample=True,\n",
    "                               temperature_degradation=0.90,\n",
    "                               top_k=None,\n",
    "                               return_probs = True,\n",
    "                               scheduler = 'cosine',\n",
    "                              )\n",
    "    return index_sample, probs, candidates\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # gt = torch.from_numpy(batch['image'].transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "    # mask = torch.from_numpy(batch['mask'].transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "    gt, _ = get_data(53, 256)\n",
    "    x = gt.to(device)\n",
    "    \n",
    "    #############################\n",
    "    mask = box_mask(x.shape, x.device, 0.8, det=True).float()\n",
    "    # mask = torch.from_numpy(BatchRandomMask(x.shape[0], x.shape[-1], hole_range=[0.5,0.6])).to(x.device)\n",
    "#     for i in range(200):\n",
    "#         bmask = box_mask(x.shape, x.device, 0.05, det=False).float()\n",
    "#         mask = torch.logical_and(mask, bmask).float()\n",
    "    ##################################\n",
    "    mask = torch.round(mask).to(device)\n",
    "    \n",
    "    VQModel, Encoder, Transformer = model.helper_model\n",
    "    VQModel = VQModel.to(device)\n",
    "    Encoder = Encoder.to(device)\n",
    "    Transformer = Transformer.to(device)\n",
    "\n",
    "    gt_quant_z, _, _ = VQModel.encode(x)\n",
    "    \n",
    "    quant_z, _, info, mask_out = Encoder.encode(x*mask, mask)\n",
    "    mask_out = mask_out.reshape(x.shape[0], -1)\n",
    "    z_indices = info[2].reshape(x.shape[0], -1)\n",
    "    batch = {'image': (x*mask).permute(0,2,3,1)}\n",
    "    \n",
    "    z_indices_complete, probs, candidates = forward_to_indices(Transformer, batch, z_indices, mask_out)\n",
    "    B, C, H, W = quant_z.shape\n",
    "    quant_z_complete = VQModel.quantize.get_codebook_entry(z_indices_complete.reshape(-1).int(), shape=(B, H, W, C))\n",
    "\n",
    "    # quant_z_complete = gt_quant_z\n",
    "    \n",
    "    mask_out_interpolate = F.interpolate(mask, (16,16)).reshape(x.shape[0], -1)\n",
    "    \n",
    "    # rec, _, _, quant_z, _, _ = model(batch, recomposition=True, mask=mask, simple_return=False)\n",
    "    # rec_fstg = VQModel.decode(quant_z)\n",
    "\n",
    "    rec_fstg = VQModel.decode(quant_z_complete)\n",
    "    \n",
    "    dec, _, mout, f0, f1 = model.current_model(batch, \n",
    "                                quant=quant_z_complete, \n",
    "                                mask_in=mask, \n",
    "                                mask_out=mask_out.reshape(B, 1, H, W),\n",
    "                                return_fstg=False, debug=True)  \n",
    "    rec = x * mask + dec * (1-mask) \n",
    "    \n",
    "\n",
    "mask_int = F.interpolate(F.interpolate(mask, size=(16,16), mode='nearest'),(256,256))\n",
    "mask_out_int = F.interpolate(mask_out.reshape(1,1,16,16),(256,256))\n",
    "\n",
    "# imshow([to_img(x*mask), to_img(rec_fstg)])\n",
    "imshow([to_img(x*mask), to_img(rec)])\n",
    "# print((1-mout).sum(), torch.sum((f1[0]**2).sum(0).reshape(-1)) / (1-mout).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_probs = np.concatenate([p.reshape(-1) for p in probs])\n",
    "    all_cands = np.concatenate([c.reshape(-1) for c in candidates])\n",
    "    replace = all_cands[np.argsort(all_probs)[100:]]\n",
    "\n",
    "    z_indices_complete[0,replace] = 0\n",
    "    mask_out_2 = (z_indices_complete != 0).float()\n",
    "    \n",
    "    z_indices_complete, _, _ = forward_to_indices(Transformer, batch, z_indices_complete.int(), mask_out_2)\n",
    "    B, C, H, W = quant_z.shape\n",
    "    quant_z_complete = VQModel.quantize.get_codebook_entry(z_indices_complete.reshape(-1).int(), shape=(B, H, W, C))\n",
    "    rec_fstg = VQModel.decode(quant_z_complete)\n",
    "    dec, _ = model.current_model(batch, \n",
    "                            quant=quant_z_complete, \n",
    "                            mask_in=mask,\n",
    "                            mask_out=mask_out.reshape(B, 1, H, W),\n",
    "                            return_fstg=False,\n",
    "                            debug=False)\n",
    "    \n",
    "    rec2 = x * mask + dec * (1-mask) \n",
    "    imshow([to_img(x*mask), to_img(rec), to_img(rec2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [p.reshape(-1) for p in probs]\n",
    "cs = [c.reshape(-1) for c in candidates]\n",
    "\n",
    "canvas = np.zeros([16**2,3])\n",
    "canvasB = np.zeros([16**2,3])\n",
    "step = 0\n",
    "for p, c in zip(ps,cs):\n",
    "    for p0, c0 in zip(p,c):\n",
    "        canvas[c0,0] = 1.0\n",
    "        canvasB[c0,0] = p0\n",
    "    if step >= end:\n",
    "        break\n",
    "    step += 1\n",
    "        \n",
    "canvas = (canvas * 255).astype(np.uint8) \n",
    "canvas = canvas.reshape(16,16,3)\n",
    "canvasB = (canvasB * 255).astype(np.uint8) \n",
    "canvasB = canvasB.reshape(16,16,3)\n",
    "end += 1\n",
    "imshow([canvas, canvasB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea40e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow([to_img(x*mask),to_img(x*mask_out), to_img(rec_fstg), to_img(rec)])\n",
    "print(np.unique(mask_int.detach().cpu().numpy()))\n",
    "imshow([to_img(x*mask), to_img(x*mask_out_int), to_img(x*(mask_int>0.25))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d36e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gt_quant_z, _, _ = VQModel.encode(x)\n",
    "    m = mask_out.reshape(1,1,16,16)\n",
    "    quant_compose = (1-m) * gt_quant_z + m * quant_z\n",
    "    rec_gt = VQModel.decode(gt_quant_z)\n",
    "    rec_cls = VQModel.decode(quant_compose)\n",
    "\n",
    "imshow([to_img(rec_gt), to_img(rec_cls)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(gt, mask, m):\n",
    "    x = gt.to(device)\n",
    "    mask_in = mask.to(device)\n",
    "    batch = {'image': gt.permute(0,2,3,1)}\n",
    "    rec, _ = m(batch, recomposition=False, mask=mask_in, simple_return=True)\n",
    "\n",
    "    # linear blending\n",
    "    k = 3\n",
    "    kernel = torch.ones(1,1,k,k) / k**2\n",
    "    pad = k // 2\n",
    "    smoothed_mask = F.conv2d(F.pad(mask_in,(pad,pad,pad,pad),value=1), kernel.to(mask_in.device), bias=None, padding=0)\n",
    "    smoothed_mask = mask_in\n",
    "    \n",
    "    # composition\n",
    "    rec = smoothed_mask * gt + (1 - smoothed_mask) * rec\n",
    "    return rec\n",
    "\n",
    "gt, mask = get_data(4, 256)\n",
    "rst = predict(gt, mask, model)\n",
    "imshow([to_img(gt*mask), to_img(rst)], titles=['input','pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3559794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramid blending? doesn't seem to work...\n",
    "import cv2 as cv\n",
    "\n",
    "A = to_img(x*mask)\n",
    "B = to_img(rec)\n",
    "M = mask[0].permute(1,2,0).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "# generate Gaussian pyramid for A\n",
    "G = A.copy()\n",
    "gpA = [G]\n",
    "for i in range(6):\n",
    " G = cv.pyrDown(G)\n",
    " gpA.append(G)\n",
    "    \n",
    "# generate Gaussian pyramid for B\n",
    "G = B.copy()\n",
    "gpB = [G]\n",
    "for i in range(6):\n",
    " G = cv.pyrDown(G)\n",
    " gpB.append(G)\n",
    "    \n",
    "# generate Laplacian Pyramid for A\n",
    "lpA = [gpA[5]]\n",
    "for i in range(5,0,-1):\n",
    " GE = cv.pyrUp(gpA[i])\n",
    " L = cv.subtract(gpA[i-1],GE)\n",
    " lpA.append(L)\n",
    "    \n",
    "# generate Laplacian Pyramid for B\n",
    "lpB = [gpB[5]]\n",
    "for i in range(5,0,-1):\n",
    " GE = cv.pyrUp(gpB[i])\n",
    " L = cv.subtract(gpB[i-1],GE)\n",
    " lpB.append(L)\n",
    "    \n",
    "# Now add left and right halves of images in each level\n",
    "LS = []\n",
    "for la,lb in zip(lpA,lpB):\n",
    " rows,cols,dpt = la.shape\n",
    " currM = cv2.resize(M,la.shape[:2]).astype(np.uint8)[...,None]\n",
    " ls = currM * la + (1-currM) * lb\n",
    " LS.append(ls)\n",
    "# now reconstruct\n",
    "ls_ = LS[0]\n",
    "for i in range(1,6):\n",
    " ls_ = cv.pyrUp(ls_)\n",
    " ls_ = cv.add(ls_, LS[i])\n",
    "\n",
    "imshow([B, M * A + (1-M) * ls_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7a8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
